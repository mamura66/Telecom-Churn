{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem: \n",
    "***Get insights from the dataset of No-Churn Telecom, to find out why the more Customers are leaving the company than expected and what can be done to improve the current situation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "- In this notebook we use the Processed Data that we have transformed from Raw Data and built a Machine Learning Model.\n",
    "- Here we use \"No-Churn_Telecom_Europe_churnFlag_only_final01.xlsx\"\n",
    "\n",
    "**Steps in Train_Model**\n",
    "\n",
    "Step 1 : Import the libraries\n",
    "\n",
    "Step 2 : Import the Processed data-set\n",
    "\n",
    "Step 3 : Split the Processed data-set\n",
    "\n",
    "Step 4 : Try Different Machine Learning Model\n",
    "\n",
    "Step 5 : Select the Model,Hypertune it and Train it\n",
    "\n",
    "Step 6 : Export the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np  #NumPy is the fundamental package for scientific computing with Python.\n",
    "import pandas as pd #andas is for data manipulation and analysis. \n",
    "import matplotlib.pyplot as plt #Matplotlib is a Python 2D plotting library which produces publication quality figures.\n",
    "import seaborn as sns #Seaborn is a Python data visualization library based on matplotlib\n",
    "%matplotlib inline\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Import the Processed data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4617, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>International_Plan</th>\n",
       "      <th>International_calls</th>\n",
       "      <th>International_Mins</th>\n",
       "      <th>VMail_Plan</th>\n",
       "      <th>Total_charges</th>\n",
       "      <th>CustServ_Calls</th>\n",
       "      <th>churn_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>75.56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13.7</td>\n",
       "      <td>1</td>\n",
       "      <td>59.24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12.2</td>\n",
       "      <td>0</td>\n",
       "      <td>62.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0</td>\n",
       "      <td>66.80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.09</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   International_Plan  International_calls  International_Mins  VMail_Plan  \\\n",
       "0                   0                    3                10.0           1   \n",
       "1                   0                    3                13.7           1   \n",
       "2                   0                    5                12.2           0   \n",
       "3                   1                    7                 6.6           0   \n",
       "4                   1                    3                10.1           0   \n",
       "\n",
       "   Total_charges  CustServ_Calls  churn_flag  \n",
       "0          75.56               1           0  \n",
       "1          59.24               1           0  \n",
       "2          62.29               0           0  \n",
       "3          66.80               2           1  \n",
       "4          52.09               3           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.set_option('display.height', 500)\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "df = pd.read_excel(\"No-Churn_Telecom_Europe_churnFlag_only_final01.xlsx\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Split the Processed data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits\n",
    "target_name = 'churn_flag'\n",
    "X = df.drop('churn_flag', axis=1)\n",
    "y = df[target_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3601, 6)\n",
      "(1016, 6)\n",
      "(3601,)\n",
      "(1016,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.22, random_state=123, stratify=y)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : Try Different Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Import Different Models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm, tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import joblib \n",
    "\n",
    "# Python script for confusion matrix creation. \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "model1 = xgboost.XGBClassifier()\n",
    "classifiers.append(model1)\n",
    "model2 = svm.SVC()\n",
    "classifiers.append(model2)\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "classifiers.append(model3)\n",
    "model4 = RandomForestClassifier()\n",
    "classifiers.append(model4)\n",
    "model5 = KNeighborsClassifier()\n",
    "classifiers.append(model5)\n",
    "model6 =GaussianNB()\n",
    "classifiers.append(model6)\n",
    "model7 =MLPClassifier(alpha=1, max_iter=1000)\n",
    "classifiers.append(model7)\n",
    "model8 = AdaBoostClassifier()\n",
    "classifiers.append(model8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1) is 0.9881889763779528\n",
      "Accuracy of SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False) is 0.9438976377952756\n",
      "Accuracy of DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best') is 0.9832677165354331\n",
      "Accuracy of RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) is 0.9901574803149606\n",
      "Accuracy of KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "                     weights='uniform') is 0.9517716535433071\n",
      "Accuracy of GaussianNB(priors=None, var_smoothing=1e-09) is 0.9478346456692913\n",
      "Accuracy of MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False) is 0.9438976377952756\n",
      "Accuracy of AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None) is 0.9940944881889764\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred= clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy of %s is %s\"%(clf, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 : Select the Model,Hypertune it and Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99797107 0.99805928 0.99947071 0.99982357 0.99620677 0.99558927\n",
      " 0.99475124 0.99802752 0.99743119 0.99973475]\n",
      "Accuracy: 1.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Using 10 fold Cross-Validation to train our RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model4 = RandomForestClassifier()\n",
    "scoring = 'roc_auc'\n",
    "#The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its \n",
    "#best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n",
    "scores = cross_val_score(model4 ,X, y, cv=10,scoring=scoring )\n",
    "print(scores)\n",
    "#The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[956   3]\n",
      " [  8  49]]\n",
      "Accuracy Score : 0.9891732283464567\n",
      "---classification_report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       959\n",
      "           1       0.94      0.86      0.90        57\n",
      "\n",
      "    accuracy                           0.99      1016\n",
      "   macro avg       0.97      0.93      0.95      1016\n",
      "weighted avg       0.99      0.99      0.99      1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print('Confusion Matrix :')\n",
    "print(confusion_matrix(y_test, rf.predict(X_test)))\n",
    "print( 'Accuracy Score :',accuracy_score(y_test, rf.predict(X_test)) )\n",
    "print( '---classification_report---')\n",
    "print(classification_report(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99858857 0.99894143 0.99920607 0.99823571 0.99673606 0.9969566\n",
      " 0.99276641 0.99678899 0.99844037 0.99938108]\n",
      "Accuracy: 1.00 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "# Using 10 fold Cross-Validation to train our  XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model1 = xgboost.XGBClassifier()\n",
    "scoring = 'roc_auc'\n",
    "#The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its \n",
    "#best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n",
    "scores = cross_val_score(model1 ,X, y, cv=10,scoring=scoring)\n",
    "print(scores)\n",
    "#The mean score and the 95% confidence interval of the score estimate are\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Confusion Matrix---\n",
      "[[952   7]\n",
      " [  5  52]]\n",
      "\n",
      "\n",
      "Accuracy Score :--> 0.9881889763779528\n",
      "\n",
      "\n",
      "---classification_report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       959\n",
      "           1       0.88      0.91      0.90        57\n",
      "\n",
      "    accuracy                           0.99      1016\n",
      "   macro avg       0.94      0.95      0.95      1016\n",
      "weighted avg       0.99      0.99      0.99      1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost Forest Model\n",
    "import xgboost\n",
    "xgb = xgboost.XGBClassifier(max_depth=16,#Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample\n",
    "                     n_estimators=100,\n",
    "                     learning_rate=0.05,\n",
    "                     booster='gbtree',\n",
    "                     subsample=1, # regularization parameter,Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "                     colsample_bylevel=1,\n",
    "                     colsample_bynode=1,\n",
    "                     colsample_bytree=1, # it works better than other two,\n",
    "                     min_child_weight=0, # can be 1,10,100 etc it parctically works,\n",
    "                     reg_alpha= 0,\n",
    "                     reg_lambda=1,\n",
    "                     gamma=0,\n",
    "                    scale_pos_weight=1,\n",
    "                    objective='binary:logistic',\n",
    "                        seed=1)\n",
    "xgb.fit(X_train, y_train)\n",
    "print('---Confusion Matrix---')\n",
    "print(confusion_matrix(y_test, xgb.predict(X_test)))\n",
    "print('\\n')\n",
    "print( 'Accuracy Score :-->',accuracy_score(y_test, xgb.predict(X_test)) )\n",
    "print('\\n')\n",
    "print( '---classification_report---')\n",
    "print(classification_report(y_test, xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations:\n",
    "- In our passage of creating the Machine Learning Model, we found that (after using 10 k-fold cross validation) **XGBoost Classifier() algorithm proves to the winner among others**, both in terms of accuracy and speed.\n",
    "- We **use k-fold cross validation because it is more accurate estimate of out-of-sample accuracy** and More \"efficient\" use of data (every observation is used for both training and testing) than train/test split.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data with SMOTE\n",
    "- In Machine Learning and Data Science we often come across a term called Imbalanced Data Distribution, generally happens when observations in one of the class are much higher or lower than the other classes.\n",
    "- Standard ML techniques such as **Logistic Regression,Decision Tree, Logistic Regression, XGBoost** have a bias towards the majority class, and they tend to ignore the minority class. They tend only to predict the majority class,\n",
    "- In more technical words, if we have imbalanced data distribution in our dataset then our model becomes more prone to the case when minority class has negligible or very lesser **Recall**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SMOTE use Euclidean distance in its calculations, it is always better to fisrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After min max Scaling : \n",
      " [[0.         0.15       0.5        1.         0.71879268 0.11111111]\n",
      " [0.         0.15       0.685      1.         0.49590276 0.11111111]\n",
      " [0.         0.25       0.61       0.         0.53755804 0.        ]\n",
      " ...\n",
      " [0.         0.15       0.65       1.         0.49808795 0.11111111]\n",
      " [0.         0.15       0.715      1.         0.49617591 0.        ]\n",
      " [0.         0.2        0.605      0.         0.47869435 0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "  \n",
    "\"\"\" MIN MAX SCALER \"\"\"\n",
    "  \n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range =(0, 1)) \n",
    "  \n",
    "# Scaled feature \n",
    "X = min_max_scaler.fit_transform(X) \n",
    "  \n",
    "print (\"\\nAfter min max Scaling : \\n\",X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3601, 6)\n",
      "(1016, 6)\n",
      "(3601,)\n",
      "(1016,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.22, random_state=123, stratify=y)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 201\n",
      "Before OverSampling, counts of label '0': 3400 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, the shape of train_X: (6800, 6)\n",
      "After OverSampling, the shape of train_y: (6800,) \n",
      "\n",
      "After OverSampling, counts of label '1': 3400\n",
      "After OverSampling, counts of label '0': 3400\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 123) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Confusion Matrix---\n",
      "[[955   4]\n",
      " [  3  54]]\n",
      "\n",
      "\n",
      "Accuracy Score :--> 0.9931102362204725\n",
      "\n",
      "\n",
      "---classification_report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       959\n",
      "           1       0.93      0.95      0.94        57\n",
      "\n",
      "    accuracy                           0.99      1016\n",
      "   macro avg       0.96      0.97      0.97      1016\n",
      "weighted avg       0.99      0.99      0.99      1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost Forest Model\n",
    "import xgboost\n",
    "xgb_after_smote = xgboost.XGBClassifier(max_depth=11, n_estimators=600,\n",
    "                     learning_rate=0.1,\n",
    "                     booster='gbtree',\n",
    "                     subsample=1, # regularization parameter,Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "                     colsample_bylevel=1,\n",
    "                     colsample_bynode=1,\n",
    "                     colsample_bytree=1, # it works better than other two,\n",
    "                     min_child_weight=0, # can be 1,10,100 etc it parctically works,\n",
    "                     reg_alpha= 0,\n",
    "                     reg_lambda=1,\n",
    "                     gamma=0,\n",
    "                    scale_pos_weight=1,\n",
    "                    objective='binary:logistic',\n",
    "                        seed=1)\n",
    "xgb_after_smote.fit(X_train_res, y_train_res)\n",
    "print('---Confusion Matrix---')\n",
    "print(confusion_matrix(y_test, xgb_after_smote.predict(X_test)))\n",
    "print('\\n')\n",
    "print( 'Accuracy Score :-->',accuracy_score(y_test, xgb_after_smote.predict(X_test)) )\n",
    "print('\\n')\n",
    "print( '---classification_report---')\n",
    "print(classification_report(y_test, xgb_after_smote.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98268398 0.99134199 0.98917749 0.99350649 0.98701299 0.98701299\n",
      " 0.98051948 0.98698482 0.99132321 0.99566161]\n",
      "Accuracy: 0.99 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Using 10 fold Cross-Validation to train our  XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model1 = xgboost.XGBClassifier(max_depth=11, n_estimators=600,\n",
    "                     learning_rate=0.1,\n",
    "                     booster='gbtree',\n",
    "                     subsample=1, # regularization parameter,Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "                     colsample_bylevel=1,\n",
    "                     colsample_bynode=1,\n",
    "                     colsample_bytree=1, # it works better than other two,\n",
    "                     min_child_weight=0, # can be 1,10,100 etc it parctically works,\n",
    "                     reg_alpha= 0,\n",
    "                     reg_lambda=1,\n",
    "                     gamma=0,\n",
    "                    scale_pos_weight=1,\n",
    "                    objective='binary:logistic',\n",
    "                        seed=1)\n",
    "\n",
    "#The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its \n",
    "#best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n",
    "scores = cross_val_score(model1 ,X, y, cv=10,scoring='f1_micro')\n",
    "print(scores)\n",
    "#The mean score and the 95% confidence interval of the score estimate are\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and fit using all the Dataset\n",
    "- This is done not to loose any given information in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 258\n",
      "Before OverSampling, counts of label '0': 4359 \n",
      "\n",
      "After OverSampling, the shape of train_X: (8718, 6)\n",
      "After OverSampling, the shape of train_y: (8718,) \n",
      "\n",
      "After OverSampling, counts of label '1': 4359\n",
      "After OverSampling, counts of label '0': 4359\n"
     ]
    }
   ],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1))) \n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y == 0))) \n",
    "  \n",
    "# import SMOTE module from imblearn library \n",
    "# pip install imblearn (if you don't have imblearn in your system) \n",
    "from imblearn.over_sampling import SMOTE \n",
    "sm = SMOTE(random_state = 123) \n",
    "X_res, y_res = sm.fit_sample(X, y.ravel()) \n",
    "  \n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_res.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_res.shape)) \n",
    "  \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_res == 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=11,\n",
       "              min_child_weight=0, missing=None, n_estimators=600, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost Forest Model\n",
    "import xgboost\n",
    "xgb_after_smote_final = xgboost.XGBClassifier(max_depth=11, n_estimators=600,\n",
    "                     learning_rate=0.1,\n",
    "                     booster='gbtree',\n",
    "                     subsample=1, # regularization parameter,Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "                     colsample_bylevel=1,\n",
    "                     colsample_bynode=1,\n",
    "                     colsample_bytree=1, # it works better than other two,\n",
    "                     min_child_weight=0, # can be 1,10,100 etc it parctically works,\n",
    "                     reg_alpha= 0,\n",
    "                     reg_lambda=1,\n",
    "                     gamma=0,\n",
    "                    scale_pos_weight=1,\n",
    "                    objective='binary:logistic',\n",
    "                        seed=1)\n",
    "xgb_after_smote_final.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_after_smote_final.predict([[1.        , 0.23174211, 0.61196845, 0.        , 0.38022451,\n",
    "        0.51498247]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    " After Using SMOTE Technique & 10 fold Cross-Validation\n",
    "- Accuracy of XGBClassifier() is after  0.99 (+/- 0.01)\n",
    "- Recall of Minority class(i.e Yes churn_flag) id 95%\n",
    "- Precision of Minority class(i.e Yes churn_flag) id 93%\n",
    "\n",
    "Since our focus class is Minority class(i.e Yes churn_flag), we are more concerned about Recall here.\n",
    "\n",
    "### Why we use k-fold Cross-Validation?\n",
    "#### Comparing cross-validation to train/test split\n",
    "\n",
    "Advantages of cross-validation:\n",
    "\n",
    "- More accurate estimate of out-of-sample accuracy\n",
    "- More \"efficient\" use of data (every observation is used for both training and testing)\n",
    "\n",
    "Advantages of train/test split:\n",
    "\n",
    "- Runs K times faster than K-fold cross-validation\n",
    "- Simpler to examine the detailed results of the testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 : Export the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Xbgboost_after_smote_Classifier_No-Churn Telecom_predict_Churn_Flag_real.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model as a pickle in a file \n",
    "joblib.dump(xgb_after_smote_final, 'Xbgboost_after_smote_Classifier_No-Churn Telecom_predict_Churn_Flag_real.pkl')       \n",
    "#joblib.dump to serialize an object hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
